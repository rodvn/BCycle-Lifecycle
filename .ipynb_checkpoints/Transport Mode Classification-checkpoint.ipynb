{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# MSR GeoLife Data\nThe first part contains code to get your hands on the data.\nThis dataset can get really big, so the second part will contain a function to help with gathering data for exploration.\n\nThe third part will then contain code examples of what you can do with the data.\n\nThis notebook has a baseline model to classify trajectories into different vehicles."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part I. Getting Data from Blob Storage"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.storage.blob import BlockBlobService\nimport os\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport sklearn",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!mkdir geolife",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "mkdir: cannot create directory ‘geolife’: File exists\r\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_path=os.getcwd() + \"/geolife\"\nblob_account_name = \"mldsdatahack2019diag\" # fill in your blob account name\nblob_account_key = \"JsauBssnY92CeD3MgI2SWhkQ16JioJCRWVW8NzKtcWckI+DaNNbCmpmMAVq27GD91mhgH+oHPx+QbIKUCow5gA==\"  # fill in your blob account key\nmycontainer = \"datahackdata2019\"       # fill in the container name \nmyblobname = \"000/Trajectory/20090705025307.csv\"        # fill in the blob name \nmydatafile = \"Output\"        # fill in the output file name",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azure\nfrom azure.storage.blob import BlockBlobService\n\nblob_service = BlockBlobService(account_name=blob_account_name, account_key=blob_account_key)\ncontainers = blob_service.list_containers()\nblobs = [a for a in blob_service.list_blobs(\"datahackdata2019\")]\ncsv_names = [a.name for a in blobs]",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for name in csv_names:\n    if name == \"BCycleAustin.csv\": continue\n    blob_service.get_blob_to_path(\"datahackdata2019\", name, os.path.join(local_path, name))",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Segregate users with and without transportation data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_dir = './geolife'\nusers = sorted([x.split(\".\")[0] for x in os.listdir(data_dir) if x.endswith(\".csv\")])",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nusers_without_transport, users_with_transport = [],[]\nfor u in users:\n    df_usr =  pd.read_csv(data_dir + '/' + u + '.csv')\n    if len(df_usr['Transportation Mode'].unique()) == 1:\n        if type(df_usr['Transportation Mode'].unique()[0]) == np.float64:\n            users_without_transport.append(u)\n            continue\n    users_with_transport.append(u)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get the relevant features for transport mode classification\nThe basic features used in this baseline are:\n- dist_x: distance travelled in longitude\n- dist_y: distance travelled in llatitude\n- time_diff: time travelled \n- avg_vel_x: average velocity in x direction(longitude)\n- avg_vel_y: average velocity in y direction(latitude)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"Data preprocessing\"\"\"\n\ndef get_cont_lab_indices(df_usr1):\n    \"\"\"\n    Returns indices at which a particular transport mode starts\n    \"\"\"\n    cont_label_indices = []\n    for idx in range(len(df_usr1)):\n        if idx == 0 or (df_usr1['Transportation Mode'][idx] != df_usr1['Transportation Mode'][idx-1]):\n            entry = []\n            entry.append(idx)\n            entry.append(df_usr1['Transportation Mode'][idx])\n            cont_label_indices.append(entry)\n    return cont_label_indices\n\ndef get_dists_and_velocities(indices, df_usr1):\n    \"\"\"\n    Function for calculating relevant velocity and distance features\n    \"\"\"\n    start,end = indices[0][0],indices[1][0]\n    dist_x = abs(df_usr1['Longitude'][end-1] - df_usr1['Longitude'][start])\n    dist_y = abs((df_usr1['Latitude'][end-1] - df_usr1['Latitude'][start]))\n    dt = pd.to_datetime(df_usr1['DateTime'])\n    time_diff = (datetime.fromtimestamp(dt[end-1].timestamp()) - datetime.fromtimestamp(dt[start].timestamp())).total_seconds()\n    avg_vel_x = dist_x/time_diff * 10000000 #arbitrary constant \n    avg_vel_y = dist_y/time_diff  * 10000000\n    return [dist_x,dist_y,time_diff, avg_vel_x, avg_vel_y]\n    \ndef get_features(user):\n    \"\"\"\n    Extact features for a particular user\n    \"\"\"\n    directory = \"data_challenge/\"\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    df_usr =  pd.read_csv(data_dir + '/' + user + '.csv')\n    df_usr1 = df_usr.dropna(subset = ['Transportation Mode']).reset_index()\n    lst = get_cont_lab_indices(df_usr1)\n    header = ['dx','dy','delta_t','vx','vy','label']\n    df = pd.DataFrame(columns = header)\n    df_idx = 0\n    for i in range(len(lst) -1):\n        if lst[i+1][0] - lst[i][0] <=1:\n            continue\n        feats = get_dists_and_velocities([lst[i],lst[i+1]], df_usr1)\n        lab = lst[i][1]\n        #ignore the entries with more than one label\n        if len(lab.split(\",\")) >=2:\n            continue\n        feats.append(lab)\n        df.loc[df_idx] = feats\n        df_idx += 1\n    df.to_csv(directory + user +\".csv\")\ntotal = len(users_with_transport)\nfor idx,user in enumerate(users_with_transport):\n    print(str(idx+1) +\"/\"+ str(total))\n    get_features(user)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1/64\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n  if self.run_code(code, result):\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "2/64\n3/64\n4/64\n5/64\n6/64\n7/64\n8/64\n9/64\n10/64\n11/64\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: RuntimeWarning: divide by zero encountered in double_scalars\n/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "12/64\n13/64\n14/64\n15/64\n16/64\n17/64\n18/64\n19/64\n20/64\n21/64\n22/64\n23/64\n24/64\n25/64\n26/64\n27/64\n28/64\n29/64\n30/64\n31/64\n32/64\n33/64\n34/64\n35/64\n36/64\n37/64\n38/64\n39/64\n40/64\n41/64\n42/64\n43/64\n44/64\n45/64\n46/64\n47/64\n48/64\n49/64\n50/64\n51/64\n52/64\n53/64\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: RuntimeWarning: invalid value encountered in double_scalars\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "54/64\n55/64\n56/64\n57/64\n58/64\n59/64\n60/64\n61/64\n62/64\n63/64\n64/64\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def merge_dfs(users):\n    \"\"\"\n    Merge user data frames into a single one\n    \"\"\"\n    directory = \"data_challenge/\"\n    combined_df = pd.DataFrame()\n    for user in users_with_transport:\n        df_usr =  pd.read_csv(directory + user + '.csv' , header = None)\n        combined_df = pd.concat([combined_df, df_usr[1:]])\n    combined_df.to_csv(directory + \"combined.csv\", index= False)\n    return combined_df\ncdf = merge_dfs(users_with_transport).reset_index()\n       ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"Random forest classifier\"\"\"\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nle = LabelEncoder()\nY = cdf.iloc[:,7]\nle.fit(Y)\nprint(list(le.classes_))\nY  = le.transform(Y)  #label encoding\nX = cdf.iloc[:,2:7]\nX   = X.as_matrix().astype(float)\nnans = []\n\n#removal of NaN and Infs \nfor idx,i in enumerate(X):\n    for ikx,j in enumerate(i):\n        if not np.isfinite(j):\n            nans.append(idx)\nnans = np.unique(np.asarray(nans))\nX = np.delete(X, nans, axis = 0)\nY =  np.delete(Y, nans)   \nassert len(X) == len(Y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=0)\nclf=RandomForestClassifier(n_estimators=500)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint(\"Accuracy:\")\nprint((y_pred == y_test).mean() * 100)\na,b = np.unique(y_test, return_counts=True)\nprint(\"Trivial Accuracy:\")\nprint(np.max(b)/np.sum(b) * 100)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['airplane', 'bike', 'boat', 'bus', 'car', 'motorcycle', 'run', 'subway', 'taxi', 'train', 'walk']\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Accuracy:\n71.4359238291302\nTrivial Accuracy:\n41.945445187853835\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.5",
      "language": "python"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "version": "3.5.5",
      "file_extension": ".py",
      "name": "python",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}